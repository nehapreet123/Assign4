{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984e1909",
   "metadata": {},
   "source": [
    "# WEB SCRAPING – ASSIGNMENT 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc890246",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date \n",
    "E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "460242ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#import debugger\n",
    "import re\n",
    "\n",
    "#import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#import time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2bdd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea00e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the webpage\n",
    "url = (\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f570ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list for scraping data\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "Date = []\n",
    "Views = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64127bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views (in Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[7]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Sto</td>\n",
       "      <td>7,046,700,000</td>\n",
       "      <td>November 2, 2020</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Despacito\"[10]</td>\n",
       "      <td>Luis F</td>\n",
       "      <td>2,993,700,000</td>\n",
       "      <td>August 4, 2017</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"See You Again\"[23]</td>\n",
       "      <td>Wiz Kha</td>\n",
       "      <td>2,894,000,000</td>\n",
       "      <td>July 10, 2017</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Gangnam Style\"⁂[31]</td>\n",
       "      <td></td>\n",
       "      <td>803,700,000</td>\n",
       "      <td>November 24, 2012</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Baby\"*[69]</td>\n",
       "      <td>Justin Bi</td>\n",
       "      <td>245,400,000</td>\n",
       "      <td>July 16, 2010</td>\n",
       "      <td>February 19, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>\"Myspace – The Movie\"‡[97][98]</td>\n",
       "      <td>eg</td>\n",
       "      <td>2,700,000</td>\n",
       "      <td>January 31, 2006</td>\n",
       "      <td>February 18, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>\"Phony Photo Booth\"‡[101]</td>\n",
       "      <td>mugen</td>\n",
       "      <td>3,400,000</td>\n",
       "      <td>December 1, 2005</td>\n",
       "      <td>January 21, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>\"The Chronic of Narnia Rap\"‡[107]</td>\n",
       "      <td>youtube</td>\n",
       "      <td>2,300,000</td>\n",
       "      <td>December 18, 2005</td>\n",
       "      <td>January 9, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>\"Ronaldinho: Touch of Gold\"‡*[110]</td>\n",
       "      <td>Nikeso</td>\n",
       "      <td>255,000</td>\n",
       "      <td>October 21, 2005</td>\n",
       "      <td>October 31, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>\"I/O Brush\"‡*[116]</td>\n",
       "      <td>la</td>\n",
       "      <td>247,000</td>\n",
       "      <td>October 5, 2005</td>\n",
       "      <td>October 29, 2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Rank  \\\n",
       "0                 \"Baby Shark Dance\"[7]   \n",
       "1                       \"Despacito\"[10]   \n",
       "2                   \"See You Again\"[23]   \n",
       "3                  \"Gangnam Style\"⁂[31]   \n",
       "4                           \"Baby\"*[69]   \n",
       "..                                  ...   \n",
       "139      \"Myspace – The Movie\"‡[97][98]   \n",
       "140           \"Phony Photo Booth\"‡[101]   \n",
       "141   \"The Chronic of Narnia Rap\"‡[107]   \n",
       "142  \"Ronaldinho: Touch of Gold\"‡*[110]   \n",
       "143                  \"I/O Brush\"‡*[116]   \n",
       "\n",
       "                                        Name         Artist  \\\n",
       "0    Pinkfong Baby Shark - Kids' Songs & Sto  7,046,700,000   \n",
       "1                                     Luis F  2,993,700,000   \n",
       "2                                    Wiz Kha  2,894,000,000   \n",
       "3                                               803,700,000   \n",
       "4                                  Justin Bi    245,400,000   \n",
       "..                                       ...            ...   \n",
       "139                                       eg      2,700,000   \n",
       "140                                    mugen      3,400,000   \n",
       "141                                  youtube      2,300,000   \n",
       "142                                   Nikeso        255,000   \n",
       "143                                       la        247,000   \n",
       "\n",
       "           Upload Date Views (in Billions)  \n",
       "0     November 2, 2020       June 17, 2016  \n",
       "1       August 4, 2017    January 12, 2017  \n",
       "2        July 10, 2017       April 6, 2015  \n",
       "3    November 24, 2012       July 15, 2012  \n",
       "4        July 16, 2010   February 19, 2010  \n",
       "..                 ...                 ...  \n",
       "139   January 31, 2006   February 18, 2006  \n",
       "140   December 1, 2005    January 21, 2006  \n",
       "141  December 18, 2005     January 9, 2006  \n",
       "142   October 21, 2005    October 31, 2005  \n",
       "143    October 5, 2005    October 29, 2005  \n",
       "\n",
       "[144 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scraping rank of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]'):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "\n",
    "#Scraping name of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]'):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"-\")\n",
    "\n",
    "#Scraping Artist of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[3]'):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"-\")\n",
    "\n",
    "#Scraping Date of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[4]'):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append(\"-\")\n",
    "\n",
    "#Scraping Views of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]'):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"-\")\n",
    "    \n",
    "# creating DataFrame for scraped data\n",
    "Wiki = pd.DataFrame({})\n",
    "Wiki['Rank'] = Rank\n",
    "Wiki['Name'] = Name\n",
    "Wiki['Artist'] = Artist\n",
    "Wiki['Upload Date'] = Date\n",
    "Wiki['Views (in Billions)'] = Views\n",
    "\n",
    "# removing stray numbers from Name column\n",
    "Wiki.Name = Wiki.Name.apply(lambda x:x[:-4].strip('\"'))\n",
    "Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29ccb312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 144 144 144 144\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank),\n",
    "len(Name),\n",
    "len(Artist),\n",
    "len(Date),\n",
    "len(Views))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc10be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd8c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56628c37",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bfaebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb86ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=(\"https://www.bcci.tv/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "905b1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "bttn=driver.find_element(By.XPATH,'//div[@class=\"right-nav\"]/a')\n",
    "driver.get(bttn.get_attribute(\"href\"))\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty lists for scraping the data\n",
    "Match_Title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91837b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>ICC MENS T20 WORLD CUP 2024</td>\n",
       "      <td>New York</td>\n",
       "      <td>5 JUNE, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>ICC MENS T20 WORLD CUP 2024</td>\n",
       "      <td>New York</td>\n",
       "      <td>9 JUNE, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>ICC MENS T20 WORLD CUP 2024</td>\n",
       "      <td>New York</td>\n",
       "      <td>12 JUNE, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>ICC MENS T20 WORLD CUP 2024</td>\n",
       "      <td>Florida</td>\n",
       "      <td>15 JUNE, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare</td>\n",
       "      <td>6 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare</td>\n",
       "      <td>7 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare</td>\n",
       "      <td>10 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare</td>\n",
       "      <td>13 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare</td>\n",
       "      <td>14 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                       Series     Place           Date  \\\n",
       "0    1st T20I  ICC MENS T20 WORLD CUP 2024  New York   5 JUNE, 2024   \n",
       "1    2nd T20I  ICC MENS T20 WORLD CUP 2024  New York   9 JUNE, 2024   \n",
       "2    3rd T20I  ICC MENS T20 WORLD CUP 2024  New York  12 JUNE, 2024   \n",
       "3    4th T20I  ICC MENS T20 WORLD CUP 2024   Florida  15 JUNE, 2024   \n",
       "4    1st T20I  INDIA TOUR OF ZIMBABWE 2024    Harare   6 JULY, 2024   \n",
       "5    2nd T20I  INDIA TOUR OF ZIMBABWE 2024    Harare   7 JULY, 2024   \n",
       "6    3rd T20I  INDIA TOUR OF ZIMBABWE 2024    Harare  10 JULY, 2024   \n",
       "7    4th T20I  INDIA TOUR OF ZIMBABWE 2024    Harare  13 JULY, 2024   \n",
       "8    5th T20I  INDIA TOUR OF ZIMBABWE 2024    Harare  14 JULY, 2024   \n",
       "\n",
       "          Time  \n",
       "0  8:00 PM IST  \n",
       "1  8:00 PM IST  \n",
       "2  8:00 PM IST  \n",
       "3  8:00 PM IST  \n",
       "4  8:00 PM IST  \n",
       "5  8:00 PM IST  \n",
       "6  8:00 PM IST  \n",
       "7  8:00 PM IST  \n",
       "8  8:00 PM IST  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"tags-wrap\"]/span[@class=\"matchOrderText ng-binding ng-scope t20-tag\"]'):\n",
    "    Match_Title.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]'):\n",
    "    Series.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"match-place ng-scope\"]/span[2]'):\n",
    "    Place.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"match-date-info\"]'):\n",
    "    Date.append(i.text.replace('\\n',' '))\n",
    "\n",
    "date=[i.split(' ',3)[:3] for i in Date]\n",
    "date=[' '.join(i) for i in date]\n",
    "Time=[i.split(' ',3)[-1] for i in Date]\n",
    "\n",
    "# creating data frame\n",
    "fixture=pd.DataFrame({'Match Title': Match_Title,\n",
    "                          \"Series\": Series,\n",
    "                          \"Place\": Place,\n",
    "                          \"Date\": date,\n",
    "                          \"Time\": Time})\n",
    "fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0504033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4aa8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca99559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705911eb",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details: A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ddfb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#import debugger\n",
    "import re\n",
    "\n",
    "#import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#import time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec56f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ae86ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the page of given url\n",
    "url=(\"http://statisticstimes.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "316436ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on economy button\n",
    "ebtn=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/button').click()\n",
    "\n",
    "#Clicking on INDIA button\n",
    "ibtn=driver.find_element(By.XPATH,'//div[@class=\"dropdown-content\"]/a[3]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d91b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cad2f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b96c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"]/tbody/tr/td[1]'):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "    \n",
    "#Scraping State\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"]/tbody/tr/td[2]'):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"-\")\n",
    "    \n",
    "#Scraping GSDP current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"]/tbody/tr/td[3]'):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"-\")\n",
    "    \n",
    "#Scraping GSDP current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"]/tbody/tr/td[4]'):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"-\")\n",
    "    \n",
    "#Scraping Share\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"]/tbody/tr/td[5]'):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"-\")\n",
    "    \n",
    "#Scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"]/tbody/tr/td[6]'):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"-\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3100629f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>3,108,022</td>\n",
       "      <td>13.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>2,700,109</td>\n",
       "      <td>2,364,514</td>\n",
       "      <td>2,071,286</td>\n",
       "      <td>8.78%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>2,500,733</td>\n",
       "      <td>2,269,995</td>\n",
       "      <td>1,978,094</td>\n",
       "      <td>8.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>2,547,861</td>\n",
       "      <td>2,258,040</td>\n",
       "      <td>1,975,595</td>\n",
       "      <td>8.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>2,230,609</td>\n",
       "      <td>1,928,683</td>\n",
       "      <td>8.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>29</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>-</td>\n",
       "      <td>35,670</td>\n",
       "      <td>31,519</td>\n",
       "      <td>0.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>30</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>35,555</td>\n",
       "      <td>31,420</td>\n",
       "      <td>0.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>31</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>30,957</td>\n",
       "      <td>26,924</td>\n",
       "      <td>0.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>24,293</td>\n",
       "      <td>0.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>9,209</td>\n",
       "      <td>0.04%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP at current price (19-20)  \\\n",
       "0     1                Maharashtra                             -   \n",
       "1     2                 Tamil Nadu                     2,700,109   \n",
       "2     3                  Karnataka                     2,500,733   \n",
       "3     4              Uttar Pradesh                     2,547,861   \n",
       "4     5                    Gujarat                             -   \n",
       "..  ...                        ...                           ...   \n",
       "61   29                     Sikkim                             -   \n",
       "62   30          Arunachal Pradesh                             -   \n",
       "63   31                   Nagaland                             -   \n",
       "64   32                    Mizoram                             -   \n",
       "65   33  Andaman & Nicobar Islands                             -   \n",
       "\n",
       "   GSDP at current price (18-19)      Share GDP($ billion)  \n",
       "0                              -  3,108,022         13.17%  \n",
       "1                      2,364,514  2,071,286          8.78%  \n",
       "2                      2,269,995  1,978,094          8.38%  \n",
       "3                      2,258,040  1,975,595          8.37%  \n",
       "4                      2,230,609  1,928,683          8.17%  \n",
       "..                           ...        ...            ...  \n",
       "61                        35,670     31,519          0.15%  \n",
       "62                        35,555     31,420          0.15%  \n",
       "63                        30,957     26,924          0.13%  \n",
       "64                             -     24,293          0.12%  \n",
       "65                             -      9,209          0.04%  \n",
       "\n",
       "[66 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating DataFrame from Scrapped Data\n",
    "GDP = pd.DataFrame({})\n",
    "GDP['Rank'] = Rank\n",
    "GDP['State'] = State\n",
    "GDP['GSDP at current price (19-20)'] = GSDP1\n",
    "GDP['GSDP at current price (18-19)'] = GSDP2\n",
    "GDP['Share'] = Share\n",
    "GDP['GDP($ billion)'] = GDP_billion\n",
    "GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80d98d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e7fe154",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b47940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "776c4b41",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2331f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#import debugger\n",
    "import re\n",
    "\n",
    "#import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#import time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85cd629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91b94670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the webpage of given url\n",
    "url=(\"https://github.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efa6578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting explore button and clicking on it\n",
    "explore = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/main/div[1]/nav/div/a[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c16a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting trending option\n",
    "trend_url = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/main/div[1]/nav/div/a[3]')\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40f6cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Language = []\n",
    "lang = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be2116d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each repository\n",
    "repository = driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]//a')\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "# scraping Repository title data\n",
    "title = driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]')\n",
    "for i in title:\n",
    "    repository_title.append(i.text)\n",
    "    \n",
    "# scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scraping Repository Description data \n",
    "    try:\n",
    "        desc = driver.find_element(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    \n",
    "    # scraping Languages used data\n",
    "    try:\n",
    "        for i in driver.find_elements(By.XPATH,'//span[@class=\"d-inline-block ml-0 mr-3\"]//span[2]'):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bb8fa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>princeton-nlp / SWE-agent</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zyronon / douyin</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wandb / openui</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>netease-youdao / QAnything</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intel-analytics / ipex-llm</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>infiniflow / ragflow</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>usebruno / bruno</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amlweems / xzbot</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rocketseat-education / nlw-unite-nodejs</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HardhatChad / ore</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pedroslopez / whatsapp-web.js</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>microsoft / generative-ai-for-beginners</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ixartz / SaaS-Boilerplate</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PKUFlyingPig / cs-self-learning</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>heyform / heyform</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HeyPuter / puter</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>atherosai / ui</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gethomepage / homepage</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OneUptime / oneuptime</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OpenBMB / ChatDev</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>airbnb / javascript</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>apache / superset</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>actions / runner-images</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>firebase / firebase-ios-sdk</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>jwasham / coding-interview-university</td>\n",
       "      <td>-</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Repository Title Repository Description  \\\n",
       "0                 princeton-nlp / SWE-agent                      -   \n",
       "1                          zyronon / douyin                      -   \n",
       "2                            wandb / openui                      -   \n",
       "3                netease-youdao / QAnything                      -   \n",
       "4                intel-analytics / ipex-llm                      -   \n",
       "5                      infiniflow / ragflow                      -   \n",
       "6                          usebruno / bruno                      -   \n",
       "7                          amlweems / xzbot                      -   \n",
       "8   rocketseat-education / nlw-unite-nodejs                      -   \n",
       "9                         HardhatChad / ore                      -   \n",
       "10            pedroslopez / whatsapp-web.js                      -   \n",
       "11  microsoft / generative-ai-for-beginners                      -   \n",
       "12                ixartz / SaaS-Boilerplate                      -   \n",
       "13          PKUFlyingPig / cs-self-learning                      -   \n",
       "14                        heyform / heyform                      -   \n",
       "15                         HeyPuter / puter                      -   \n",
       "16                           atherosai / ui                      -   \n",
       "17                   gethomepage / homepage                      -   \n",
       "18                    OneUptime / oneuptime                      -   \n",
       "19                        OpenBMB / ChatDev                      -   \n",
       "20                      airbnb / javascript                      -   \n",
       "21                        apache / superset                      -   \n",
       "22                  actions / runner-images                      -   \n",
       "23              firebase / firebase-ios-sdk                      -   \n",
       "24    jwasham / coding-interview-university                      -   \n",
       "\n",
       "   Language Used  \n",
       "0             []  \n",
       "1             []  \n",
       "2             []  \n",
       "3             []  \n",
       "4             []  \n",
       "5             []  \n",
       "6             []  \n",
       "7             []  \n",
       "8             []  \n",
       "9             []  \n",
       "10            []  \n",
       "11            []  \n",
       "12            []  \n",
       "13            []  \n",
       "14            []  \n",
       "15            []  \n",
       "16            []  \n",
       "17            []  \n",
       "18            []  \n",
       "19            []  \n",
       "20            []  \n",
       "21            []  \n",
       "22            []  \n",
       "23            []  \n",
       "24            []  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Framing\n",
    "Github = pd.DataFrame({})\n",
    "Github['Repository Title'] = repository_title\n",
    "Github['Repository Description'] = Description\n",
    "Github['Language Used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2037dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4eb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688735d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09254923",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board \n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc4e3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#import debugger\n",
    "import re\n",
    "\n",
    "#import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#import time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "939023f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to webdriver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87efee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting webpage of giver url\n",
    "url = (\"http://www.billboard.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b8c6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on option button charts\n",
    "charts=driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div/div/div/ul/li[1]/h3/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e25101df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Song_Name = []\n",
    "Artist_Name = []\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21aa2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting url for top 100 songs\n",
    "urls = driver.find_element(By.XPATH,'/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div/div[2]/span/a')\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "117c7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping data of song names\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[1]/h3\"):\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "# scraping data of artist names\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[1]/span\"):\n",
    "    Artist_Name.append(i.text)\n",
    "    \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[4]/span\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[5]/span\"):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[2]/ul/li[4]/ul/li[6]/span\"):\n",
    "    Weeks_on_board.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f1c6379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like That</td>\n",
       "      <td>Future, Metro Boomin &amp; Kendrick Lamar</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name                                 Artist Last Week Rank Peak Rank  \\\n",
       "0  Like That  Future, Metro Boomin & Kendrick Lamar              -         1   \n",
       "\n",
       "  Weeks on board  \n",
       "0              1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "billiboard = pd.DataFrame({})\n",
    "billiboard['Name'] = Song_Name\n",
    "billiboard['Artist'] = Artist_Name\n",
    "billiboard['Last Week Rank'] = Last_week_rank\n",
    "billiboard['Peak Rank'] = Peak_rank\n",
    "billiboard['Weeks on board'] = Weeks_on_board\n",
    "billiboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0c2f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0bb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953492f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1faf9e7a",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre \n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189f6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff86626",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36d14b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n",
    "\n",
    "\n",
    "# scraping book names data\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraping author names data\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author_name.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of volumes sold\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of publisher names\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping  data of genre\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr//td[6]\"):\n",
    "    Genre.append(i.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bc3d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name            Author  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume sold        Publisher                        Genre  \n",
       "0    5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1    4,475,152       Bloomsbury           Children's Fiction  \n",
       "2    4,200,654       Bloomsbury           Children's Fiction  \n",
       "3    4,179,479       Bloomsbury           Children's Fiction  \n",
       "4    3,758,936     Random House              Romance & Sagas  \n",
       "..         ...              ...                          ...  \n",
       "95     807,311     Random House   General & Literary Fiction  \n",
       "96     794,201          Penguin        Food & Drink: General  \n",
       "97     792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98     791,507            Orion           Biography: General  \n",
       "99     791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "Novels = pd.DataFrame({})\n",
    "Novels['Book Name'] = Book_name\n",
    "Novels['Author'] = Author_name\n",
    "Novels['Volume sold'] = Volumes_sold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels['Genre'] = Genre\n",
    "Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ef5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd60737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78407bf1",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details: \n",
    "A) Dataset name \n",
    "B) Data type \n",
    "C) Task \n",
    "D) Attribute type \n",
    "E) No of instances \n",
    "F) No of attribute G) Year \n",
    " Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29edc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#import debugger\n",
    "import re\n",
    "\n",
    "#import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#import time\n",
    "import time\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "918e8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the webpage of mentioned in url\n",
    "url = (\"https://archive.ics.uci.edu/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad89bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(By.XPATH,'//a[@class=\"btn-primary btn\"]')\n",
    "page_url = viewall_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "580e5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(By.XPATH,\"//p[@class='normal']//b/a\")\n",
    "\n",
    "urls = []\n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5c7a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c004ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping  Dataset name\n",
    "    try:\n",
    "        dataset_name = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/h2/a')\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(By.XPATH,'//p[@class=\"truncate\"]')\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(By.XPATH,'//span[@class=\"truncate\"]')\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(By.XPATH,'//span[@class=\"truncate\"]')\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Instances\n",
    "    try:\n",
    "        instances = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr/td[4]\")\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Arrtibutes\n",
    "    try:\n",
    "        attribute = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr[2]/td[4]\")\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Year\n",
    "    try:\n",
    "        year = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr[2]/td[6]\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09061a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Dataset Name, Data Type , Task , Attribute Type , No of Instance , No of Attributes , Year ]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "ML = pd.DataFrame({})\n",
    "ML['Dataset Name'] = Dataset_name \n",
    "ML['Data Type '] = Data_type\n",
    "ML['Task '] = Task \n",
    "ML['Attribute Type '] = Attribute_type \n",
    "ML['No of Instance '] = No_of_instances\n",
    "ML['No of Attributes '] = No_of_attributes \n",
    "ML['Year '] = Year \n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d80c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
